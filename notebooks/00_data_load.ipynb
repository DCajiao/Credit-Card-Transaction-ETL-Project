{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Load Proccess**:\n",
    "- Objective: This notebook presents the process of loading clean data resulting from the previous phase of the project. It is intended to use a new database due to the limitations of the free [Render database](https://github.com/DCajiao/workshop001_candidates_analysis/blob/main/docs/database/how_to_deploy_databases_on_render.md) instances.\n",
    "- **Important note**: All the documentation that you will find in this and the following notebooks is an emulation of each pipeline task that will be run in Airflow, so *we will use a sample of the data for testing purposes*, while in the pipeline we will use all the data we have. \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **First Step**: Load clean, processed and previously transformed data from a csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the 'src' folder to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/credit_card_transactions_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 1052352 rows and 21 columns\n",
      "The columns are: ['id', 'trans_date_trans_time', 'cc_num', 'merchant', 'category', 'amt', 'first', 'last', 'gender', 'street', 'city', 'state', 'zip', 'lat', 'long', 'job', 'dob', 'trans_num', 'is_fraud', 'merch_zipcode', 'age']\n"
     ]
    }
   ],
   "source": [
    "print(f'The dataset has {df.shape[0]} rows and {df.shape[1]} columns')\n",
    "print(f'The columns are: {df.columns.tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                       1052352\n",
       "trans_date_trans_time    1038039\n",
       "cc_num                       946\n",
       "merchant                     693\n",
       "category                      14\n",
       "amt                        48789\n",
       "first                        348\n",
       "last                         478\n",
       "gender                         2\n",
       "street                       946\n",
       "city                         864\n",
       "state                         49\n",
       "zip                          935\n",
       "lat                          933\n",
       "long                         934\n",
       "job                          488\n",
       "dob                          931\n",
       "trans_num                1052352\n",
       "is_fraud                       2\n",
       "merch_zipcode              28307\n",
       "age                           75\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many unique values does each column have?\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Second Step**: Upload data to database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task:\n",
    "\n",
    "- Import db class to use connector\n",
    "- Establish connection and execute the queries to create the schema and send the data.\n",
    "- Validate that the table has been created and that all records have been loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Generating schema for raw_table\n",
      "INFO:root:Infering SQL type for int64\n",
      "INFO:root:Infering SQL type for object\n",
      "INFO:root:Infering SQL type for int64\n",
      "INFO:root:Infering SQL type for object\n",
      "INFO:root:Infering SQL type for object\n",
      "INFO:root:Infering SQL type for float64\n",
      "INFO:root:Infering SQL type for object\n",
      "INFO:root:Infering SQL type for object\n",
      "INFO:root:Infering SQL type for object\n",
      "INFO:root:Infering SQL type for object\n",
      "INFO:root:Infering SQL type for object\n",
      "INFO:root:Infering SQL type for object\n",
      "INFO:root:Infering SQL type for int64\n",
      "INFO:root:Infering SQL type for float64\n",
      "INFO:root:Infering SQL type for float64\n",
      "INFO:root:Infering SQL type for object\n",
      "INFO:root:Infering SQL type for object\n",
      "INFO:root:Infering SQL type for object\n",
      "INFO:root:Infering SQL type for bool\n",
      "INFO:root:Infering SQL type for float64\n",
      "INFO:root:Infering SQL type for int64\n",
      "INFO:root:Query written to ../sql/raw_table_schema.sql\n",
      "INFO:root:Generating seed data for raw_table\n",
      "INFO:root:Query written to ../sql/raw_table_seed_data.sql\n"
     ]
    }
   ],
   "source": [
    "from utils.pysqlschema import SQLSchemaGenerator\n",
    "\n",
    "generator = SQLSchemaGenerator(table_name='raw_table')\n",
    "generator.generate_schema(df, '../sql/raw_table_schema.sql')\n",
    "generator.generate_seed_data(df, '../sql/raw_table_seed_data.sql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from connections.db import DB\n",
    "db = DB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:✔ Connected to database\n",
      "INFO:root:✔ Query executed\n",
      "INFO:root:✔ Cursor closed\n",
      "INFO:root:✔ Connection closed\n"
     ]
    }
   ],
   "source": [
    "# Remove the table if it already exists\n",
    "db.execute(\"../sql/queries/002_drop_tables.sql\", fetch_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:✔ Connected to database\n",
      "INFO:root:✔ Query executed\n",
      "INFO:root:✔ Cursor closed\n",
      "INFO:root:✔ Connection closed\n"
     ]
    }
   ],
   "source": [
    "# Create schema\n",
    "db.execute(\"../sql/raw_table_schema.sql\", fetch_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:✔ Connected to database\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed a batch of 20000 records\n",
      "INFO:root:✔ Executed the final batch of 12352 records\n",
      "INFO:root:✔ Cursor closed\n",
      "INFO:root:✔ Connection closed\n"
     ]
    }
   ],
   "source": [
    "# Seed data by executing the seed data script in batches\n",
    "db.execute_in_batches(\"../sql/raw_table_seed_data.sql\", batch_size=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:✔ Connected to database\n",
      "INFO:root:✔ Query executed\n",
      "INFO:root:✔ Cursor closed\n",
      "INFO:root:✔ Connection closed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('public.raw_table', 1034393)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query the tables to verify that the data has been inserted\n",
    "db.execute(\"../sql/queries/001_view_tables_sizes.sql\", fetch_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Results**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We extracted a sample representing 45% of the totality of our cleaned and processed data. \n",
    "2. We have created a raw table with that sample. \n",
    "3. We have uploaded the sample to generate the test to be performed on the following notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "credit-card-transaction-airflow-etl-projec-yQOlLuJu-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
