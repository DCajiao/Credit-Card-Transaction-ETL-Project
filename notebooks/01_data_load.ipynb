{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Load Proccess**:\n",
    "- Objective: This notebook presents the process of loading clean data resulting from the previous phase of the project. It is intended to use a new database due to the limitations of the free [Render database](https://github.com/DCajiao/workshop001_candidates_analysis/blob/main/docs/database/how_to_deploy_databases_on_render.md) instances.\n",
    "- **Important note**: All the documentation that you will find in this and the following notebooks is an emulation of each pipeline task that will be run in Airflow, so *we will use a sample of the data for testing purposes*, while in the pipeline we will use all the data we have. \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **First Step**: Load clean, processed and previously transformed data from a csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the 'src' folder to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/credit_card_transactions_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 1052352 rows and 21 columns\n",
      "The columns are: ['id', 'trans_date_trans_time', 'cc_num', 'merchant', 'category', 'amt', 'first', 'last', 'gender', 'street', 'city', 'state', 'zip', 'lat', 'long', 'job', 'dob', 'trans_num', 'is_fraud', 'merch_zipcode', 'age']\n"
     ]
    }
   ],
   "source": [
    "print(f'The dataset has {df.shape[0]} rows and {df.shape[1]} columns')\n",
    "print(f'The columns are: {df.columns.tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                       1052352\n",
       "trans_date_trans_time    1038039\n",
       "cc_num                       946\n",
       "merchant                     693\n",
       "category                      14\n",
       "amt                        48789\n",
       "first                        348\n",
       "last                         478\n",
       "gender                         2\n",
       "street                       946\n",
       "city                         864\n",
       "state                         49\n",
       "zip                          935\n",
       "lat                          933\n",
       "long                         934\n",
       "job                          488\n",
       "dob                          931\n",
       "trans_num                1052352\n",
       "is_fraud                       2\n",
       "merch_zipcode              28307\n",
       "age                           75\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many unique values does each column have?\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a sample of the data (45% of the rows)\n",
    "df_test = df.sample(int(df.shape[0]*0.45))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 473558 rows and 21 columns\n",
      "The columns are: ['id', 'trans_date_trans_time', 'cc_num', 'merchant', 'category', 'amt', 'first', 'last', 'gender', 'street', 'city', 'state', 'zip', 'lat', 'long', 'job', 'dob', 'trans_num', 'is_fraud', 'merch_zipcode', 'age']\n"
     ]
    }
   ],
   "source": [
    "print(f'The dataset has {df_test.shape[0]} rows and {df.shape[1]} columns')\n",
    "print(f'The columns are: {df_test.columns.tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a sample of the data (45% of the rows)\n",
    "df_test = df.sample(int(df.shape[0]*0.45))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                       473558\n",
       "trans_date_trans_time    470686\n",
       "cc_num                      943\n",
       "merchant                    693\n",
       "category                     14\n",
       "amt                       35955\n",
       "first                       347\n",
       "last                        477\n",
       "gender                        2\n",
       "street                      943\n",
       "city                        862\n",
       "state                        49\n",
       "zip                         932\n",
       "lat                         930\n",
       "long                        931\n",
       "job                         488\n",
       "dob                         928\n",
       "trans_num                473558\n",
       "is_fraud                      2\n",
       "merch_zipcode             27679\n",
       "age                          75\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the sample to a new CSV file\n",
    "df_test.to_csv('../data/credit_card_transactions_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Second Step**: Upload data to database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task:\n",
    "\n",
    "- Import db class to use connector\n",
    "- Establish connection and execute the queries to create the schema and send the data.\n",
    "- Validate that the table has been created and that all records have been loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-26 00:08:09,945 - Generating schema for dataraw_testing_table\n",
      "2024-09-26 00:08:09,946 - Infering SQL type for int64\n",
      "2024-09-26 00:08:09,948 - Infering SQL type for object\n",
      "2024-09-26 00:08:09,950 - Infering SQL type for int64\n",
      "2024-09-26 00:08:09,951 - Infering SQL type for object\n",
      "2024-09-26 00:08:09,952 - Infering SQL type for object\n",
      "2024-09-26 00:08:09,953 - Infering SQL type for float64\n",
      "2024-09-26 00:08:09,954 - Infering SQL type for object\n",
      "2024-09-26 00:08:09,955 - Infering SQL type for object\n",
      "2024-09-26 00:08:09,956 - Infering SQL type for object\n",
      "2024-09-26 00:08:09,956 - Infering SQL type for object\n",
      "2024-09-26 00:08:09,957 - Infering SQL type for object\n",
      "2024-09-26 00:08:09,958 - Infering SQL type for object\n",
      "2024-09-26 00:08:09,959 - Infering SQL type for int64\n",
      "2024-09-26 00:08:09,960 - Infering SQL type for float64\n",
      "2024-09-26 00:08:09,961 - Infering SQL type for float64\n",
      "2024-09-26 00:08:09,962 - Infering SQL type for object\n",
      "2024-09-26 00:08:09,963 - Infering SQL type for object\n",
      "2024-09-26 00:08:09,963 - Infering SQL type for object\n",
      "2024-09-26 00:08:09,964 - Infering SQL type for bool\n",
      "2024-09-26 00:08:09,964 - Infering SQL type for float64\n",
      "2024-09-26 00:08:09,965 - Infering SQL type for int64\n",
      "2024-09-26 00:08:09,968 - Query written to ../sql/sample/schema_clean.sql\n",
      "2024-09-26 00:08:09,970 - Generating seed data for dataraw_testing_table\n",
      "2024-09-26 00:08:46,561 - Query written to ../sql/sample/seed_data_clean.sql\n"
     ]
    }
   ],
   "source": [
    "from utils.pysqlschema import SQLSchemaGenerator\n",
    "\n",
    "generator = SQLSchemaGenerator(table_name='dataraw_testing_table')\n",
    "generator.generate_schema(df_test, '../sql/sample/schema_clean.sql')\n",
    "generator.generate_seed_data(df_test, '../sql/sample/seed_data_clean.sql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from connections.db import DB\n",
    "db = DB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-26 00:09:24,952 - ✔ Connected to database\n",
      "2024-09-26 00:09:25,121 - ✔ Query executed\n",
      "2024-09-26 00:09:25,122 - ✔ Cursor closed\n",
      "2024-09-26 00:09:25,122 - ✔ Connection closed\n"
     ]
    }
   ],
   "source": [
    "# Create schema\n",
    "db.execute(\"../sql/sample/schema_clean.sql\", fetch_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-26 00:09:28,177 - ✔ Connected to database\n",
      "2024-09-26 00:09:41,245 - ✔ Executed a batch of 20000 records\n",
      "2024-09-26 00:09:55,942 - ✔ Executed a batch of 20000 records\n",
      "2024-09-26 00:10:07,761 - ✔ Executed a batch of 20000 records\n",
      "2024-09-26 00:10:20,018 - ✔ Executed a batch of 20000 records\n",
      "2024-09-26 00:10:32,053 - ✔ Executed a batch of 20000 records\n",
      "2024-09-26 00:10:45,684 - ✔ Executed a batch of 20000 records\n",
      "2024-09-26 00:11:04,935 - ✔ Executed a batch of 20000 records\n",
      "2024-09-26 00:11:17,581 - ✔ Executed a batch of 20000 records\n",
      "2024-09-26 00:11:29,545 - ✔ Executed a batch of 20000 records\n",
      "2024-09-26 00:11:41,551 - ✔ Executed a batch of 20000 records\n",
      "2024-09-26 00:12:03,535 - ✔ Executed a batch of 20000 records\n",
      "2024-09-26 00:12:15,463 - ✔ Executed a batch of 20000 records\n",
      "2024-09-26 00:12:27,944 - ✔ Executed a batch of 20000 records\n",
      "2024-09-26 00:12:39,993 - ✔ Executed a batch of 20000 records\n",
      "2024-09-26 00:12:59,700 - ✔ Executed a batch of 20000 records\n",
      "2024-09-26 00:13:14,198 - ✔ Executed a batch of 20000 records\n",
      "2024-09-26 00:13:25,767 - ✔ Executed a batch of 20000 records\n",
      "2024-09-26 00:13:37,868 - ✔ Executed a batch of 20000 records\n",
      "2024-09-26 00:13:55,582 - ✔ Executed a batch of 20000 records\n",
      "2024-09-26 00:14:11,987 - ✔ Executed a batch of 20000 records\n",
      "2024-09-26 00:14:23,495 - ✔ Executed a batch of 20000 records\n",
      "2024-09-26 00:14:35,382 - ✔ Executed a batch of 20000 records\n",
      "2024-09-26 00:14:51,688 - ✔ Executed a batch of 20000 records\n",
      "2024-09-26 00:15:06,411 - ✔ Executed the final batch of 13558 records\n",
      "2024-09-26 00:15:06,412 - ✔ Cursor closed\n",
      "2024-09-26 00:15:06,415 - ✔ Connection closed\n"
     ]
    }
   ],
   "source": [
    "# Seed data by executing the seed data script in batches\n",
    "db.execute_in_batches(\"../sql/sample/seed_data_clean.sql\", batch_size=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-26 00:19:22,464 - ✔ Connected to database\n",
      "2024-09-26 00:19:22,618 - ✔ Query executed\n",
      "2024-09-26 00:19:22,619 - ✔ Cursor closed\n",
      "2024-09-26 00:19:22,620 - ✔ Connection closed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('public.dataraw_testing_table', 440000)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query the tables to verify that the data has been inserted\n",
    "db.execute(\"../sql/queries/01_view_tables_sizes.sql\", fetch_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Results**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We extracted a sample representing 45% of the totality of our cleaned and processed data. \n",
    "2. We have created a raw table with that sample. \n",
    "3. We have uploaded the sample to generate the test to be performed on the following notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "credit-card-transaction-airflow-etl-projec-8pJookSb-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
